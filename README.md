# ğŸ™ï¸ Transcription with Diarization and Confidence Scoring

This project provides a **complete audio processing pipeline** that performs:
- **Speech Transcription** (via [WhisperX](https://github.com/m-bain/whisperX))
- **Speaker Diarization** (who spoke when, via [pyannote.audio](https://github.com/pyannote/pyannote-audio))
- **Confidence Scoring** (word-level scores)
- **Audio Enhancement** (denoising, resampling)

It is containerized with **Docker** for reproducibility and uses **GPU acceleration** (CUDA) for fast inference.

---

## ğŸ–¥ï¸ System Requirements

### Hardware
- **GPU:** NVIDIA GPU with CUDA support (tested on CUDA **12.4**)
- **VRAM:** Minimum **8 GB** (16 GB+ recommended for large models like `large-v2`)
- **RAM:** 16 GB system memory or more
- **Disk:** At least 5 GB free for models, logs, and outputs

### Software
- **OS:** Ubuntu 22.04 (native or via WSL2)
- **Docker Desktop** (with WSL2 integration enabled)
- **NVIDIA Drivers** (latest, must match CUDA version)
- **NVIDIA Container Toolkit** (installed automatically by setup script)

---

## ğŸš€ Quick Start

### 1. Clone repository
```bash
git clone https://github.com/yourusername/audio-td.git
cd audio-td
2. Setup environment
Run the automated setup script:

bash
Copy code
chmod +x setup.sh
./setup.sh
This will:

Verify NVIDIA GPU availability (nvidia-smi)

Check Docker installation and user permissions

Install NVIDIA Container Toolkit for GPU passthrough

Create project folders (input/, output/, models/, logs/)

Generate .env for secrets

Build the Docker image with CUDA support

3. Configure environment variables
Hugging Face Token
Open .env (created automatically) and add your token:

env
Copy code
HUGGING_FACE_TOKEN=your_hf_token_here
ğŸ‘‰ You can create a free token at Hugging Face Settings.

Whisper Model
Choose a Whisper model size (tradeoff between speed & accuracy):

bash
Copy code
export WHISPER_MODEL=large-v2
4. Prepare your audio
Put your input file (.wav, .mp3, etc.) in the input/ directory.
Example:

bash
Copy code
input/noisy_audio.mp3
5. Run transcription with diarization
Build and run the pipeline:

bash
Copy code
docker compose build
docker compose run --rm audio-td python main.py "input/noisy_audio.mp3" --min_speakers 4 --max_speakers 5
Arguments:

--min_speakers â†’ minimum expected speakers

--max_speakers â†’ maximum expected speakers

ğŸ“‚ Outputs
After processing, check the output/ folder:

enhanced_for_asr.wav â†’ audio cleaned & resampled

original_for_diarization.wav â†’ original audio for diarization

diarized_transcription.json â†’ final structured result

Example JSON line:
json
Copy code
{
  "speaker": "SPEAKER_01",
  "word": "Hello",
  "start": "00:00.500",
  "end": "00:01.200",
  "confidence": 0.94
}
Each entry contains:

Speaker label

Word spoken

Start & end timestamps (mm:ss.sss format)

Confidence score

ğŸ› ï¸ Development
Project Structure
bash
Copy code
â”œâ”€â”€ setup.sh              # Automated setup script
â”œâ”€â”€ docker-compose.yml    # Docker configuration
â”œâ”€â”€ Dockerfile            # Base image and environment
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ main.py               # Main pipeline (recommended entry point)
â”œâ”€â”€ main_01.py            # Alternative pipeline (simpler version)
â”œâ”€â”€ src/                  # Core processing modules
â”‚   â”œâ”€â”€ asr_pipeline.py       # Automatic Speech Recognition pipeline
â”‚   â”œâ”€â”€ audio_enhancer.py     # Noise reduction & audio enhancement
â”‚   â””â”€â”€ diarization_pipeline.py # Speaker diarization pipeline
â”œâ”€â”€ input/                # Place your input audio files here
â”œâ”€â”€ output/               # Generated outputs
â”œâ”€â”€ models/               # Downloaded ML models
â”œâ”€â”€ logs/                 # Runtime logs
â””â”€â”€ .env                  # Environment variables (Hugging Face token)
main.py vs main_01.py
main.py â†’ full pipeline with enhanced error handling, JSONL output (line-by-line JSON), and flexible speaker constraints.

main_01.py â†’ simpler reference pipeline with fewer options, outputs a single JSON array.
ğŸ‘‰ Use main.py by default unless testing alternative behavior.

Dependencies
Defined in requirements.txt:

Core ML: torch, torchaudio, whisperx, pyannote.audio

Audio Processing: pydub, librosa, demucs, speechbrain

Data & Utils: datasets, pandas, huggingface-hub, tqdm

Visualization/UI: matplotlib, gradio

âš ï¸ Notes & Tips
GPU is mandatory for performance â€” CPU-only mode is not supported for long audios.

Adjust --min_speakers / --max_speakers for more accurate diarization.

Large Whisper models (large-v2) need â‰¥16GB VRAM.

.gitignore ensures input/, output/, and .env are not tracked.

For debugging, check logs inside logs/.

ğŸ“œ License
MIT License (update if needed)

ğŸ™Œ Acknowledgements
OpenAI Whisper

WhisperX

pyannote.audio

Speechbrain

yaml
Copy code

---

âš¡ Now your **`src/` folder and pipeline scripts are clearly documented** in the Project Structure section.  

Do you also want me to add **flow diagrams** (like how audio goes through `audio_enhancer â†’ asr_pipeline â†’ diarization_p
